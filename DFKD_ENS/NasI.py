import importlib
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import random
import os
import argparse

import sys
sys.path.append("../")
from utils import seed_everything, logger, getDataloader
from datetime import datetime
from collections import namedtuple
from utils.logger import logger
from val_model import val_model
from models.model_dict import model_dict
from models.rs_model.Generator import Generator_v
from models.rs_model.Gating import Gating
from distutils.util import strtobool

parser = argparse.ArgumentParser(description="Pytorch CTR Teacher Training")
parser.add_argument('--dataset', default='ml_1m', choices=['ml_1m'])
parser.add_argument('--student', default='DNN', choices=['DeepFM', 'DNN', 'DCN'])
parser.add_argument('--test_teacher', default=True, type=bool, help='Whether to test the teacher model before training (In practice, the testing behavior cannot be carried out)')
parser.add_argument('--emb_size', default=16, type=int, help='embedding vector dimension of the feature')
parser.add_argument('--k', default=4, type=int, help='factor dimensions in FM')
parser.add_argument('--hd', default=[16, 16], type=int, help='hidden dimensions in DNN')
parser.add_argument('--epoch', default=400, type=int, metavar='N', help='number of total epochs to run')
parser.add_argument('--pretrain_epoch', default=0, type=int, metavar='N', help='number of generator pretrain epochs to run')
parser.add_argument('--epoch_s', default=200, type=int, metavar='N', help='number of epochs a student is trained in a large round')
parser.add_argument('--epoch_g', default=50, type=int, metavar='N', help='number of epochs a generator is trained in a large round')
parser.add_argument('-b', '--batch_size', default=2048, type=int, metavar='N', help='mini-batch size in validation/test phase')
parser.add_argument('--lr_s', '--learning_rate_s', default=1e-3, type=float, metavar='LR', help='initial student learning rate', dest='lr_s')
parser.add_argument('--lr_g_pre', '--learning_rate_g_pre', default=1e-4, type=float, metavar='LR', help='pretrained generator learning rate', dest='lr_g_pre')
parser.add_argument('--lr_g', '--learning_rate_g', default=1e-4, type=float, metavar='LR', help='initial generator learning rate', dest='lr_g')
parser.add_argument('--lr_w', '--learning_rate_w', default=1e-3, type=float, metavar='LR', help='initial gating learning rate', dest='lr_w')
parser.add_argument('--mean_pre', default=None, type=float, help='the mean of the generator distributed pre-training method')
parser.add_argument('--std_pre', default=None, type=float, help='the standard deviation of the generator distributed pre-training method')
parser.add_argument('--weight_decay_s', default=1e-5, type=float, help='student weight decay')
parser.add_argument('--weight_decay_g_pre', default=1e-5, type=float, help='pretrained generator weight decay')
parser.add_argument('--weight_decay_g', default=1e-5, type=float, help='generator weight decay')
parser.add_argument('--weight_decay_w', default=1e-5, type=float, help='gating weight decay') 
parser.add_argument('--lr_decay_milestones', default='40', type=str, help='milestones for student learning rate decay')
parser.add_argument('--decay_gamma', default=0.3, type=float, help='student learning rate decay gamma')
parser.add_argument('--gen_batch_size', default=25, type=int, help='number of samples generated by the generator per round')
parser.add_argument('--polar_t', default=.7, type=float, help='temperature used to regulate the polarization phenomenon in gating')
parser.add_argument('--dp_rate', default=.3, type=float, help='dropout rate used to regulate the polarization phenomenon in gating')
parser.add_argument('--kd_t', default=1., type=float, help='distillation temperature')
parser.add_argument('--ens_method', default='adaptive', choices=['adaptive', 'mean'], help='allocation method of teacher weight')
parser.add_argument('--alpha', default=1., type=float, help='weight values of the loss are distilled using logits')
parser.add_argument('--beta', default=0.001, type=float, help='weight values to balance the sample loss')
parser.add_argument('--gpu', default=0, type=int,  help='GPU id to use')
parser.add_argument('--seed', default=0, type=int, help='seed for initializing training')
parser.add_argument('-j', '--workers', default=4, type=int, metavar='N', help='number of data loading workers (default: 2)')
parser.add_argument('--std_t', default=0.2, type=float)
parser.add_argument('--setting_name', default="settings", type=str)
parser.add_argument('--val_epoch', default=1, type=int)
parser.add_argument('--norm_weight', default=0.05, type=float)
parser.add_argument('--upper_limit', default=0.5, type=float)
parser.add_argument('--sample_one', default=False, type=lambda x: bool(strtobool(x)))
parser.add_argument('--data_path', default='../data')

def main():
    ### Seetings
    args = parser.parse_args()

    experimet_project = f'experiment_{datetime.now().date()}'

    if not os.path.exists(experimet_project):
        os.makedirs(experimet_project)

    save_path = f"./{ experimet_project }/{ args.dataset }_{ args.student }_{datetime.now().strftime('%Y%m%d%H%M%S')}"
    if not os.path.exists(save_path):
        os.makedirs(save_path)
    _logger = logger(experimet_project)

    _logger(f'{ args }')
    _logger("-------------------------------")

    if args.seed is not None:
        seed_everything(args.seed)
        _logger(f"seed: { args.seed }")

    device = 'cpu'
    if args.gpu is not None and args.gpu < torch.cuda.device_count():
        device = torch.device(f'cuda:{args.gpu}')
        _logger(f'Use { device }')

    ### Dataset
    opt = namedtuple("variables", ['dataset', 'type', 'batch_size', 'shuffle', 'num_workers'])
    dataloader = getDataloader(dataset_name=args.dataset, data_path=args.data_path)
    val_loader, _, _ = dataloader.val_dataloader(batch_size=args.batch_size, shuffle=True, num_workers=args.workers)
    test_loader, _, _ = dataloader.test_dataloader(batch_size=args.batch_size, shuffle=True, num_workers=args.workers)

    feature_count = {
        'user_feature': None,
        'item_feature': None
    }

    given_dims = {
        'user_feature': None,
        'item_feature': None
    }

    setting_name = args.setting_name
    setting = importlib.import_module(setting_name)
    teacher_model_name = getattr(setting, "teacher_model_name")
    teacher_model_path_dict = getattr(setting, "teacher_model_path_dict")

    ### Teachers
    _logger("teachers' seeting")
    teacher_list = []

    for teacher in teacher_model_name:
        model_name, _ = teacher.split('_')
        t_ckpt = torch.load(teacher_model_path_dict[teacher])['model_state_dict']
        _logger(f"{ teacher }:{ teacher_model_path_dict[teacher] }")

        feature_count['user_feature'] = t_ckpt['embeddings.user_feature.weight'].size(0)
        feature_count['item_feature'] = t_ckpt['embeddings.item_feature.weight'].size(0)
        given_dims['user_feature'] = t_ckpt['embeddings.user_feature.weight'].size(1)
        given_dims['item_feature'] = t_ckpt['embeddings.item_feature.weight'].size(1)
        
        k = None
        if 'twoDeg_v' in t_ckpt:
            k = t_ckpt['twoDeg_v'].size(1)
        
        hidden_dim = []
        layer = 1
        while f'linear_{ layer }.weight' in t_ckpt:
            hidden_dim.append(t_ckpt[f'linear_{ layer }.weight'].size(0))
            layer += 1

        params = {
            'features': feature_count,
            'device': device,
            'given_dims': given_dims,
            'k': k,
            'hidden_dim': hidden_dim,
        }
        teacher_model = model_dict[model_name](params)
        teacher_model.load_state_dict(t_ckpt)
        teacher_model.to(device)

        teacher_model.eval()
        teacher_list.append(teacher_model)

        if args.test_teacher:
            _, auc, logloss = val_model(teacher_model, val_loader, device)
            _logger(f"val_auc: { auc }, val_logloss: { logloss }")
            _, auc, logloss = val_model(teacher_model, test_loader, device)
            _logger(f"test_auc: { auc }, test_logloss: { logloss }")
            _logger('--------------')

    ### Student
    given_dims['user_feature'], given_dims['item_feature'] = args.emb_size, args.emb_size
    params = {
        'features': feature_count,
        'device': device,
        'given_dims': given_dims,
        'k': args.k,
        'hidden_dim': args.hd
    }
    student = model_dict[args.student](params)
    student.to(device)
    optim_s = optim.Adam(filter(lambda p: p.requires_grad, student.parameters()), lr=args.lr_s, weight_decay=args.weight_decay_s)
    milestones = [ int(ms) for ms in args.lr_decay_milestones.split(',') ]
    sched_s = optim.lr_scheduler.MultiStepLR(optim_s, milestones=milestones, gamma=args.decay_gamma)

    ### Generator
    gen_batch_size = args.gen_batch_size
    noise = torch.rand(1024, 100).to(device)

    generator = Generator_v(feature_count['user_feature'], feature_count['item_feature'], noise.size(1))
    generator.to(device)
    pre_optim_g = optim.Adam(filter(lambda p: p.requires_grad, generator.parameters()), lr=args.lr_g_pre, weight_decay=args.weight_decay_g_pre)
    optim_g = optim.Adam(filter(lambda p: p.requires_grad, generator.parameters()), lr=args.lr_g, weight_decay=args.weight_decay_g)

    ### Gaing
    weight_gating = Gating(len(teacher_list), gen_batch_size, args.polar_t, args.dp_rate)
    weight_gating.to(device)
    optim_w = optim.Adam(filter(lambda p: p.requires_grad, weight_gating.parameters()), lr=args.lr_w, weight_decay=args.weight_decay_w)

    ### Generator pretrain
    set_label = torch.linspace(0, 1, steps=args.gen_batch_size, device=device)
    _set_label = torch.linspace(0, 1, steps=21, device=device)
    generator.train()
    generator.pretrain_target_labels(teacher_list, pre_optim_g, set_label, args.pretrain_epoch, noise)

    ### Adversarial KD
    features = {
        'user_feature': None,
        'item_feature': None
    }
    best_auc = 0
    total_epoch = args.epoch
    total_epoch_g = args.epoch_g
    total_epoch_s = args.epoch_s
    method = args.ens_method
    T = args.kd_t
    alpha = args.alpha
    beta = args.beta
    loss_fn = nn.L1Loss()


    for epoch in range(total_epoch):

        set_label, _ = torch.sort(set_label)

        current_lr = optim_s.param_groups[0]['lr']
        _logger(f'student_lr: { current_lr }')
        _logger(f"set label = {set_label}")

        # train student
        # '''
        student.train()
        generator.eval()

        if method == 'adaptive':
            weight_gating.train()
        else:
            weight_gating.eval()

        train_loss_s = 0

        if args.sample_one:
            with torch.no_grad():
                noise = torch.rand(gen_batch_size, 100).to(device)
                features['user_feature'], features['item_feature'] = generator(noise)
                t_logit_list = []
                for teacher in teacher_list:
                    t_logit = torch.sigmoid(teacher(features, discrete=False) / T)
                    t_logit_list.append(t_logit.detach())
                t_logit_list = torch.stack(t_logit_list, dim=0)

        for epoch_s in range(total_epoch_s):

            if args.sample_one == False:
                if epoch_s % 1 == 0:
                    with torch.no_grad():
                        noise = torch.rand(gen_batch_size, 100).to(device)
                        features['user_feature'], features['item_feature'] = generator(noise)
                        t_logit_list = []
                        for teacher in teacher_list:
                            t_logit = torch.sigmoid(teacher(features, discrete=False) / T)
                            t_logit_list.append(t_logit.detach())
                        t_logit_list = torch.stack(t_logit_list, dim=0)

            if method == 'adaptive':
                optim_s.zero_grad()
                if epoch_s % 5 == 0:
                    optim_w.zero_grad()
                s_logit = torch.sigmoid(student(features, discrete=False) / T)

                scale = weight_gating(t_logit_list)
                t_logit = torch.sum(torch.stack([weight * tensor for weight, tensor in zip(scale, t_logit_list)]), dim=0)
                loss_pentaly = 0
                if (scale == 0).any() == False:
                    loss_pentaly = F.relu(torch.max(scale) - args.upper_limit)
                loss_s = loss_fn(s_logit, t_logit) + loss_pentaly

                loss_s.backward()
                optim_s.step()
                if epoch_s % 5 == 0:
                    optim_w.step()
                
            elif method == 'mean':
                optim_s.zero_grad()
                s_logit = torch.sigmoid(student(features, discrete=False) / T)
                t_logit = t_logit_list.mean(dim=0)
                loss_s = loss_fn(s_logit, t_logit) 
                loss_s.backward()
                optim_s.step()
            
            indices = torch.searchsorted(_set_label, t_logit) - 1


            if epoch_s % 50 == 0:
                _logger(f"[{ epoch_s + 1 } / { total_epoch_s }] loss_s: { loss_s.item() } ")

            train_loss_s += loss_s.item()


        train_loss_s /= total_epoch_s
        
        _logger(f"[{ epoch + 1 } / { total_epoch }] train_loss_s: { train_loss_s } ")
        if method == 'adaptive':
            _logger(f"[{ epoch + 1 } / { total_epoch }] last_scale: { scale } ")
        sched_s.step()

        # train generator
        student.eval()
        generator.train()
        # 冻结门控
        weight_gating.eval()

        train_loss_g = 0
        features_1 = {
            'user_feature': None,
            'item_feature': None
        }
        features_2 = {
            'user_feature': None,
            'item_feature': None
        }
        for epoch_g in range(total_epoch_g):
            optim_g.zero_grad()

            noises_1 = torch.rand(gen_batch_size // 2, 100).to(device)
            noises_2 = torch.rand(gen_batch_size // 2, 100).to(device)
            features_1['user_feature'], features_1['item_feature'] = generator(noises_1)
            features_cat_1 = torch.cat((features_1['user_feature'], features_1['item_feature']), dim=1)

            features_2['user_feature'], features_2['item_feature'] = generator(noises_2)
            features_cat_2 = torch.cat((features_2['user_feature'], features_2['item_feature']), dim=1)

            features['user_feature'] = torch.cat((features_1['user_feature'], features_2['user_feature']), dim=0)
            features['item_feature'] = torch.cat((features_1['item_feature'], features_2['item_feature']), dim=0)

            s_logit = torch.sigmoid(student(features, discrete=False) / T)
            
            t_logit_list_1 = []
            for idx, teacher in enumerate(teacher_list):
                t_logit = torch.sigmoid(teacher(features_1, discrete=False) / T)
                t_logit_list_1.append(t_logit)
            t_logit_list_1 = torch.stack(t_logit_list_1, dim=0)
            
            t_logit_list_2 = []
            for idx, teacher in enumerate(teacher_list):
                t_logit = torch.sigmoid(teacher(features_2, discrete=False) / T)
                t_logit_list_2.append(t_logit)
            t_logit_list_2 = torch.stack(t_logit_list_2, dim=0)

            lz = torch.norm(features_cat_1 - features_cat_2) / torch.norm(noises_1 - noises_2)
            loss_diversity_seeking = 1 / (lz + 1 * 1e-20)

            if method == 'adaptive':
                t_logit_save = torch.cat((t_logit_list_1, t_logit_list_2), dim=-1)
                if gen_batch_size % 2 == 0: # 偶数
                    t_logit_list = t_logit_save
                else: # 奇数, 需要额外补上一个维度
                    t_logit_list = torch.cat((t_logit_save, t_logit_save.mean(dim=-1, keepdim=True)), dim=-1)
                scale = weight_gating(t_logit_list)
                t_logit = torch.sum(torch.stack([weight * tensor for weight, tensor in zip(scale, t_logit_save)]), dim=0)
            elif method == 'mean':
                # t_logit = t_logit_list.mean(dim=0)
                t_logit_1 = t_logit_list_1.mean(dim=0)
                t_logit_2 = t_logit_list_2.mean(dim=0)
                t_logit = torch.cat((t_logit_1, t_logit_2), dim=0)

            def entropy_loss(predictions):
                # 计算预测输出的熵
                epsilon = 1e-6  # 防止 log(0)
                entropy = -predictions * torch.log(predictions + epsilon) - (1 - predictions) * torch.log(1 - predictions + epsilon)
                return torch.mean(entropy)
            # penalty = -entropy_loss(t_logit_sorted)
            # loss_hint = get_hint_loss(student, teacher_list, features, discrete=False, epoch=epoch)
            loss_g = -alpha * (loss_fn(s_logit, t_logit)) + beta * loss_diversity_seeking + args.norm_weight * (torch.norm(features['user_feature'], 2) + torch.norm(features['item_feature'], 2)) 
            # + beta * loss_fn(t_logit_sorted, set_label) 
            # + 0.01 * L_DIV
            # + 0. * loss_bn
            # beta * loss_fn(t_logit_sorted, set_label) + 0 * loss_bn
            loss_g.backward()
            optim_g.step()

            if epoch_g % 50 == 0:
                _logger(f"[{ epoch_g + 1 } / { total_epoch_g }] loss_g: { loss_g.item() } ")
        
            
            train_loss_g += loss_g.item()

        train_loss_g /= total_epoch_g
        _logger(f'[{epoch + 1} / {total_epoch}] train_loss_g: { train_loss_g }')

        if method == 'adaptive':
            _logger(f"[{ epoch + 1 } / { total_epoch }] frozen_scale: { scale } ")

        if epoch % args.val_epoch == 0:
            student.eval()
            generator.eval()
            val_loss, val_auc, val_logloss = val_model(student, val_loader, device)
            _logger(f'[{ epoch + 1 } / { total_epoch }] val_loss: { val_loss }')
            _logger(f'[{ epoch + 1 } / { total_epoch }] val_auc: { val_auc }, val_logloss: { val_logloss }')
            _logger('------------------------')

            if val_auc > best_auc:
                best_auc = val_auc
                torch.save({
                    'model_state_dict': student.state_dict(),
                    'optim_state_dict': optim_s.state_dict(),
                    'sched_state_dict': sched_s.state_dict(),
                    # 'gen_state_dict': generator.state_dict()
                }, f'{ save_path }/student_model.pt')
        
    s_ckpt = torch.load(f'{ save_path }/student_model.pt')
    student.load_state_dict(s_ckpt['model_state_dict'])
    _, val_auc, val_logloss = val_model(student, val_loader, device)
    _logger(f'val_auc: { val_auc }, val_logloss: { val_logloss }')
    _, test_auc, test_logloss = val_model(student, test_loader, device)
    _logger(f'test_auc: { test_auc }, test_logloss: { test_logloss }')
    _logger(f'best_auc: { best_auc }')
    _logger(f"alpha = { args.alpha }, beta = { args.beta }, norm_weight = { args.norm_weight }")

if __name__ == '__main__':
    main()
