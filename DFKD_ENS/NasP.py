import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import random
import os
import argparse

import sys
sys.path.append("../")

from utils import seed_everything, logger, getDataloader
from val_model import val_model

from datetime import datetime
from collections import namedtuple
from utils.logger import logger

from models.model_dict import model_dict
from models.rs_model.Generator import Generator
from models.rs_model.Gating import Gating
from settings import teacher_model_name, teacher_model_path_dict

parser = argparse.ArgumentParser(description="DFKD")
parser.add_argument('--dataset', default='ml_1m', choices=['ml_1m'])
parser.add_argument('--data_path', default='../data')
parser.add_argument('--student', default='DNN', choices=['DeepFM', 'DNN', 'DCN'])
parser.add_argument('--test_teacher', default=True, type=bool, help='Whether to test the teacher model before training (In practice, the testing behavior cannot be carried out)')
parser.add_argument('--emb_size', default=16, type=int, help='embedding vector dimension of the feature')
parser.add_argument('--k', default=4, type=int, help='factor dimensions in FM')
parser.add_argument('--hd', default=[16, 16], type=int, help='hidden dimensions in DNN')
parser.add_argument('--epoch', default=80, type=int, metavar='N', help='number of total epochs to run')
parser.add_argument('--pretrain_epoch', default=100, type=int, metavar='N', help='number of generator pretrain epochs to run')
parser.add_argument('--epoch_s', default=300, type=int, metavar='N', help='number of epochs a student is trained in a large round')
parser.add_argument('--epoch_g', default=100, type=int, metavar='N', help='number of epochs a generator is trained in a large round')
parser.add_argument('-b', '--batch_size', default=2048, type=int, metavar='N', help='mini-batch size in validation/test phase')
parser.add_argument('--lr_s', '--learning_rate_s', default=1e-3, type=float, metavar='LR', help='initial student learning rate', dest='lr_s')
parser.add_argument('--lr_g_pre', '--learning_rate_g_pre', default=1e-1, type=float, metavar='LR', help='pretrained generator learning rate', dest='lr_g_pre')
parser.add_argument('--lr_g', '--learning_rate_g', default=5e-3, type=float, metavar='LR', help='initial generator learning rate', dest='lr_g')
parser.add_argument('--lr_w', '--learning_rate_w', default=1e-3, type=float, metavar='LR', help='initial gating learning rate', dest='lr_w')
parser.add_argument('--mean_pre', default=None, type=float, help='the mean of the generator distributed pre-training method')
parser.add_argument('--std_pre', default=None, type=float, help='the standard deviation of the generator distributed pre-training method')
parser.add_argument('--weight_decay_s', default=1e-5, type=float, help='student weight decay')
parser.add_argument('--weight_decay_g_pre', default=1e-5, type=float, help='pretrained generator weight decay')
parser.add_argument('--weight_decay_g', default=1e-5, type=float, help='generator weight decay')
parser.add_argument('--weight_decay_w', default=1e-5, type=float, help='gating weight decay')
parser.add_argument('--lr_decay_milestones', default='80', type=str, help='milestones for student learning rate decay')
parser.add_argument('--decay_gamma', default=0.1, type=float, help='student learning rate decay gamma')
parser.add_argument('--gen_batch_size', default=25, type=int, help='number of samples generated by the generator per round')
parser.add_argument('--polar_t', default=.9, type=float, help='temperature used to regulate the polarization phenomenon in gating')
parser.add_argument('--dp_rate', default=.1, type=float, help='dropout rate used to regulate the polarization phenomenon in gating')
parser.add_argument('--kd_t', default=1., type=float, help='distillation temperature')
parser.add_argument('--ens_method', default='adaptive', choices=['adaptive', 'mean'], help='allocation method of teacher weight')
parser.add_argument('--alpha', default=1., type=float, help='weight values of the loss are distilled using logits')
parser.add_argument('--beta', default=0., type=float, help='weight values to balance the sample loss')
parser.add_argument('--gpu', default=0, type=int,  help='GPU id to use')
parser.add_argument('--seed', default=None, type=int, help='seed for initializing training')
parser.add_argument('-j', '--workers', default=2, type=int, metavar='N', help='number of data loading workers (default: 2)')


def main():
    ### settings
    args = parser.parse_args()

    experimet_project = f'experiment_{datetime.now().date()}'

    if not os.path.exists(experimet_project):
        os.makedirs(experimet_project)

    save_path = f"./{ experimet_project }/{ args.dataset }_{ args.student }_{datetime.now().strftime('%Y%m%d%H%M%S')}"
    if not os.path.exists(save_path):
        os.makedirs(save_path)
    _logger = logger(experimet_project)
    _logger(f'{ args }')
    _logger("-------------------------------")

    if args.seed is not None:
        seed_everything(args.seed)
        _logger(log_str=f"seed: { args.seed }")

    device = 'cpu'
    if args.gpu is not None and args.gpu < torch.cuda.device_count():
        device = torch.device(f'cuda:{args.gpu}')
        _logger(log_str=f'Use { device }')

    opt = namedtuple("variables", ['dataset', 'type', 'batch_size', 'shuffle', 'num_workers'])
    dataloader = getDataloader(dataset_name=args.dataset, data_path=args.data_path)
    val_loader, _, _ = dataloader.val_dataloader(batch_size=args.batch_size, shuffle=True, num_workers=args.workers)
    test_loader, _, _ = dataloader.test_dataloader(batch_size=args.batch_size, shuffle=True, num_workers=args.workers)

    feature_count = {
        'user_feature': None,
        'item_feature': None
    }

    given_dims = {
        'user_feature': None,
        'item_feature': None
    }

    ### Teachers
    _logger("teachers' setting")
    teacher_list = []
    for teacher in teacher_model_name:
        model_name, _ = teacher.split('_')
        t_ckpt = torch.load(teacher_model_path_dict[teacher])['model_state_dict']
        _logger(f"{ teacher }:{ teacher_model_path_dict[teacher] }")

        feature_count['user_feature'] = t_ckpt['embeddings.user_feature.weight'].size(0)
        feature_count['item_feature'] = t_ckpt['embeddings.item_feature.weight'].size(0)
        given_dims['user_feature'] = t_ckpt['embeddings.user_feature.weight'].size(1)
        given_dims['item_feature'] = t_ckpt['embeddings.item_feature.weight'].size(1)
        
        k = None
        if 'twoDeg_v' in t_ckpt:
            k = t_ckpt['twoDeg_v'].size(1)
        
        hidden_dim = []
        layer = 1
        while f'linear_{ layer }.weight' in t_ckpt:
            hidden_dim.append(t_ckpt[f'linear_{ layer }.weight'].size(0))
            layer += 1

        params = {
            'features': feature_count,
            'device': device,
            'given_dims': given_dims,
            'k': k,
            'hidden_dim': hidden_dim,
        }
        teacher_model = model_dict[model_name](params)
        teacher_model.load_state_dict(t_ckpt)
        teacher_model.to(device)
        teacher_model.eval()
        teacher_list.append(teacher_model)

        if args.test_teacher:
            _, auc, logloss = val_model(teacher_model, val_loader, device)
            _logger(log_str=f"val_auc: { auc }, val_logloss: { logloss }")
            _, auc, logloss = val_model(teacher_model, test_loader, device)
            _logger(log_str=f"test_auc: { auc }, test_logloss: { logloss }")
            _logger('--------------')

    ### Student
    given_dims['user_feature'], given_dims['item_feature'] = args.emb_size, args.emb_size
    params = {
        'features': feature_count,
        'device': device,
        'given_dims': given_dims,
        'k': args.k,
        'hidden_dim': args.hd
    }
    student = model_dict[args.student](params)
    student.to(device)
    optim_s = optim.Adam(filter(lambda p: p.requires_grad, student.parameters()), lr=args.lr_s, weight_decay=args.weight_decay_s)
    milestones = [ int(ms) for ms in args.lr_decay_milestones.split(',') ]
    sched_s = optim.lr_scheduler.MultiStepLR(optim_s, milestones=milestones, gamma=args.decay_gamma)

    ### Generator
    gen_batch_size = args.gen_batch_size
    generator = Generator(gen_batch_size, feature_count['user_feature'], feature_count['item_feature'], device)
    generator.to(device)
    pre_optim_g = optim.Adam(filter(lambda p: p.requires_grad, generator.parameters()), lr=args.lr_g_pre, weight_decay=args.weight_decay_g_pre)
    optim_g = optim.Adam(filter(lambda p: p.requires_grad, generator.parameters()), lr=args.lr_g, weight_decay=args.weight_decay_g)

    ### Gaing
    weight_gating = Gating(len(teacher_list), gen_batch_size, args.polar_t, args.dp_rate)
    weight_gating.to(device)
    optim_w = optim.Adam(filter(lambda p: p.requires_grad, weight_gating.parameters()), lr=args.lr_w, weight_decay=args.weight_decay_w)

    ### Generator pretrain
    set_label = torch.linspace(0, 1, steps=args.gen_batch_size, device=device)
    generator.train()
    if args.mean_pre == None or args.std_pre == None:
        _logger(f"The generator pre-training phase uses 'target_labels' method,\ntarget labels: { set_label }")
        generator.pretrain_target_labels(teacher_list, pre_optim_g, set_label, args.pretrain_epoch)
    else:
        _logger(f"The generator pre-training phase uses 'distribution' method,\nmean: { args.mean_pre }, stdv: { args.std_pre }")
        generator.pretrain_distribution(teacher_list, pre_optim_g, args.mean_pre, args.std_pre, args.pretrain_epoch)
    _logger("-----------------------------")

    ### Adversarial KD
    features = {
        'user_feature': None,
        'item_feature': None
    }
    best_auc = 0
    total_epoch = args.epoch
    total_epoch_g = args.epoch_g
    total_epoch_s = args.epoch_s
    method = args.ens_method
    T = args.kd_t
    alpha = args.alpha
    beta = args.beta
    loss_fn = nn.L1Loss()
    for epoch in range(total_epoch):
        
        current_lr = optim_s.param_groups[0]['lr']
        _logger(f'student_lr: { current_lr }')

        # train student
        student.train()
        generator.eval()
        generator.set_grad(need_grad=False)

        if method == 'adaptive':
            weight_gating.train()
        else:
            weight_gating.eval()

        features['user_feature'], features['item_feature'] = generator()

        t_logit_list = []
        with torch.no_grad():
            for teacher in teacher_list:
                t_logit = torch.sigmoid(teacher(features, discrete=False) / T)
                t_logit_list.append(t_logit.detach())
        t_logit_list = torch.stack(t_logit_list, dim=0)

        train_loss_s = 0
        for epoch_s in range(total_epoch_s):
            if method == 'adaptive':
                optim_s.zero_grad()
                if epoch_s % 10 == 0:
                    optim_w.zero_grad()
                s_logit = torch.sigmoid(student(features, discrete=False) / T)
                scale = weight_gating(t_logit_list)
                t_logit = torch.sum(torch.stack([weight * tensor for weight, tensor in zip(scale, t_logit_list)]), dim=0)
                t_logit_sorted, _ = torch.sort(t_logit)
                loss_s = loss_fn(s_logit, t_logit)
                loss_s.backward()
                optim_s.step()
                if epoch_s % 10 == 0:
                    optim_w.step()
                
            elif method == 'mean':
                optim_s.zero_grad()
                s_logit = torch.sigmoid(student(features, discrete=False) / T)
                t_logit = t_logit_list.mean(dim=0)
                loss_s = loss_fn(s_logit, t_logit)
                loss_s.backward()
                optim_s.step()

            if epoch_s % 50 == 0:
                _logger(f"[{ epoch_s + 1 } / { total_epoch_s }] loss_s: { loss_s.item() } ")

            train_loss_s += loss_s.item()

        train_loss_s /= total_epoch_s
        
        _logger(f"[{ epoch + 1 } / { total_epoch }] train_loss_s: { train_loss_s } ")
        if method == 'adaptive':
            _logger(f"[{ epoch + 1 } / { total_epoch }] last_scale: { scale } ")
        sched_s.step()

        # train generator
        student.eval()
        generator.train()
        generator.set_grad(need_grad=True)
        
        weight_gating.eval()

        train_loss_g = 0
        for epoch_g in range(total_epoch_g):
            optim_g.zero_grad()
            features['user_feature'], features['item_feature'] = generator()
            s_logit = torch.sigmoid(student(features, discrete=False) / T)
            t_logit_list = []
            for teacher in teacher_list:
                t_logit = torch.sigmoid(teacher(features, discrete=False) / T)
                t_logit_list.append(t_logit)
            t_logit_list = torch.stack(t_logit_list, dim=0)

            if method == 'adaptive':
                scale = weight_gating(t_logit_list)
                t_logit = torch.sum(torch.stack([weight * tensor for weight, tensor in zip(scale, t_logit_list)]), dim=0)
            elif method == 'mean':
                t_logit = t_logit_list.mean(dim=0)
            
            loss_g = -alpha * loss_fn(s_logit, t_logit)
            loss_g.backward()
            optim_g.step()

            if epoch_g % 50 == 0:
                _logger(f"[{ epoch_g + 1 } / { total_epoch_g }] loss_g: { loss_g.item() } ")
            
            train_loss_g += loss_g.item()

        train_loss_g /= total_epoch_g
        _logger(f'[{epoch + 1} / {total_epoch}] train_loss_g: { train_loss_g }')

        if method == 'adaptive':
            _logger(f"[{ epoch + 1 } / { total_epoch }] frozen_scale: { scale } ")

        student.eval()
        generator.eval()
        generator.set_grad(need_grad=False)
        val_loss, val_auc, val_logloss = val_model(student, val_loader, device)
        _logger(f'[{ epoch + 1 } / { total_epoch }] val_loss: { val_loss }')
        _logger(f'[{ epoch + 1 } / { total_epoch }] val_auc: { val_auc }, val_logloss: { val_logloss }')
        _logger('------------------------')

        if val_auc > best_auc:
            best_auc = val_auc
            torch.save({
                'model_state_dict': student.state_dict(),
                'optim_state_dict': optim_s.state_dict(),
                'sched_state_dict': sched_s.state_dict()
            }, f'{ save_path }/student_model.pt')
        
    s_ckpt = torch.load(f'{ save_path }/student_model.pt')
    student.load_state_dict(s_ckpt['model_state_dict'])
    _, val_auc, val_logloss = val_model(student, val_loader, device)
    _logger(f'val_auc: { val_auc }, logloss: { val_logloss }')
    _, test_auc, test_logloss = val_model(student, test_loader, device)
    _logger(f'test_auc: { test_auc }, test_logloss: { test_logloss }')

if __name__ == '__main__':
    main()
